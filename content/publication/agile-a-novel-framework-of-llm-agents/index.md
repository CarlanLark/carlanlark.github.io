---
abstract: We introduce a novel framework of LLM agents named AGILE (AGent that Interacts and Learns from Environments) designed to perform complex conversational tasks with users, leveraging LLMs, memory, tools, and interactions with experts. The agent's abilities include not only conversation but also reflection, utilization of tools, and consultation with experts. We formulate the construction of such an LLM agent as a reinforcement learning problem, in which the LLM serves as the policy model. We fine-tune the LLM using labeled data of actions and the PPO algorithm. We focus on question answering and release a dataset for agents called ProductQA, comprising challenging questions in online shopping. Our extensive experiments on ProductQA and MedMCQA show that AGILE agents based on 13B and 7B LLMs trained with PPO can outperform GPT-4 agents. Our ablation study highlights the indispensability of memory, tools, consultation, reflection, and reinforcement learning in achieving the agent's strong performance.
slides: ""
url_pdf: https://arxiv.org/abs/2405.14751
publication_types:
  - "3"
authors:
  - Peiyuan Feng
  - Yichen He
  - admin
  - Yuan Lin
  - Hanchong Zhang
  - Yuchen Zhang
  - Hang Li
author_notes: 
  - Equal contribution
  - Equal contribution
  - Equal contribution
  - Equal contribution
  - Equal contribution
  - Equal contribution
publication: Preprint
summary: "We introduce a novel framework of LLM agents named AGILE (AGent that Interacts and Learns from Environments) designed to perform complex conversational tasks with users, leveraging LLMs, memory, tools, and interactions with experts."
url_dataset: ""
url_project: ""
publication_short: ""
url_source: ""
url_video: ""
title: "AGILE: A Novel Framework of LLM Agents"
doi: ""
featured: true
tags: []
projects: []
image:
  caption: ""
  focal_point: ""
  preview_only: false
  filename: 8.png
date: 2024-05-24T02:12:23.523Z
url_slides: ""
publishDate: 2024-05-24T00:00:00.000Z
url_poster: ""
url_code: https://github.com/bytarnish/AGILE
---